{
  "name": "A Course on Fairness, Accuracy and Transparency in Machine Learning",
  "tagline": "Sponsored by the GIAN program of the Government of India",
  "body": "### Overview\r\n\r\nMachine learning has taken over our world, in more ways than we realize. You might get book recommendations, or an efficient route to your destination, or even a winning strategy for a game of Go. But you might also be admitted to college, granted a loan, or hired for a job based on algorithmically enhanced decision-making. We believe machines are neutral arbiters: cold, calculating entities that always make the right decision, that can see patterns that our human minds can’t or won’t. But are they? Or is decision-making-by-algorithm a way to amplify, extend and make inscrutable the biases and discrimination that is prevalent in society?\r\n\r\nTo answer these questions, we need to go back — all the way to the original ideas of justice and fairness in society. We also need to go forward — towards a mathematical framework for talking about justice and fairness in machine learning. I will talk about the growing landscape of research in algorithmic fairness: how we can reason systematically about biases in algorithms, and how we can make our algorithms fair(er).\r\n\r\n### Course Mechanics\r\nThis is a short (and intense) course. We'll cover material in two lecture chunks each day. But this is also a **discussion**, on a topic that's still very new and that has fluid boundaries and evolving formalisms. I've provided readings that are technical and non-technical in nature, and I expect (and hope!) that the presentations will provoke discussion, arguments, and new ideas. \r\n\r\nSo **please read the provided materials** ahead of the lecture and come prepared with your questions, comments and critiques. You'll benefit the most from the material if you have time to engage with it. \r\n\r\n### Syllabus\r\n*   Dec 11: **Preliminaries**\r\n\r\n      Basics of machine learning -- supervised and unsupervised learning, empirical risk minimization, classifiers, regression, training and generalization. \r\n\r\n      *Readings*:\r\n    * Chapters 1 [(PDF)](http://ciml.info/dl/v0_9/ciml-v0_9-ch01.pdf), 3.1-3.4 [(PDF)](http://ciml.info/dl/v0_9/ciml-v0_9-ch03.pdf) and 4 [(PDF)](http://ciml.info/dl/v0_9/ciml-v0_9-ch04.pdf) of Hal Daumé's [excellent book on ML](http://ciml.info). \r\n    * Hal's [post on the ML development pipeline](http://nlpers.blogspot.com/2016/08/debugging-machine-learning.html). This is framed in terms of the way errors can creep into the modeling process, but it doubles as an excellent explanation of the pipeline itself. \r\n\r\n*   Dec 12: **Automated Decision Making**\r\n\r\n      ​    Case studies of the use of machine learning in applications. An introduction to different formal notions of fairness.\r\n\r\n      ​    *Readings* (applications):\r\n\r\n    * Criminal Justice:\r\n      * Risk Assessment: ([Primer](http://www.datacivilrights.org/pubs/2015-1027/Courts_and_Predictive_Algorithms.pdf), [discussion](http://www.datacivilrights.org/pubs/2015-1027/WDN-Courts_and_Predictive_Algorithms.pdf) from Data and Civil Rights Workshop)\r\n      * Predictive Policing: ([Primer](http://www.datacivilrights.org/pubs/2015-1027/Predictive_Policing.pdf) and [discussion](http://www.datacivilrights.org/pubs/2015-1027/WDN-Predictive_Policing.pdf))\r\n      * Julia Angwin, Jeff Larson, Surya Mattu, Lauren Kirchner, [“Machine Bias\"](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing)\r\n      * [Video showing predictive policing in action](http://fusion.net/story/283896/real-future-episode-12-predictive-policing/)\r\n    * Hiring:\r\n      * [On how AI can be used in hiring](http://venturebeat.com/2016/11/09/ai-is-helping-job-candidates-bypass-resume-bias-and-black-holes/) (by one company that provides solutions). [Another perspective](http://www.ca.com/us/rewrite/articles/application-economy/can-artificial-intelligence-find-the-perfect-hire.html) \r\n      * [Predicting Voice-elicited emotions](http://delivery.acm.org/10.1145/2790000/2788619/p1969-li.pdf?ip=71.195.244.110&id=2788619&acc=OA&key=4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4D4702B0C3E38B35%2E4037F4931E565B6B&CFID=872166982&CFTOKEN=74413255&__acm__=1480927397_b769b575e3f06e480d52f70766f3a596) (from KDD 2015)\r\n    * Credit Scoring and Loans\r\n      * China's new [citizen scoring system](https://www.washingtonpost.com/world/asia_pacific/chinas-plan-to-organize-its-whole-society-around-big-data-a-rating-for-everyone/2016/10/20/1cd0dd9c-9516-11e6-ae9d-0030ac1899cd_story.html?utm_term=.f8184eeef71d)\r\n    * Education:\r\n      * [\"19 Ways Data Analysis Empowered Students and Schools\"](https://fpf.org/wp-content/uploads/2016/03/Final_19Times-Data_Mar2016-1.pdf) pages 21-26.\r\n      * [Promise and limits of learning analytics](http://www.chronicle.com.libproxy.ocean.edu:2048/article/This-Chart-Shows-the-Promise/234573)\r\n\r\n    *Readings* (notions of fairness):\r\n    * [Discrimination-aware data mining](Discrimination-aware data min- ing).  (*discriminatory classifiers*)\r\n    * [Data preprocessing techniques for classification without discrimination](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwiLtYnNit7QAhWHiVQKHcUaAE8QFggkMAE&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F1a43%2Fd5a8f3dd82a138c92911befba05ae98add27.pdf&usg=AFQjCNHwZ1vsGzJRLsbv4QoW-gLX3DIyCg&sig2=0TikurXGq184Xoqi7O6eMw).  (*statistical parity*)\r\n    * [Fairness through awareness](https://arxiv.org/abs/1104.3913).  (*individual fairness*)\r\n    * [Certifying and removing disparate impact](https://arxiv.org/abs/1412.3756) (*disparate impact*)\r\n    * [Equality of opportunity in supervised learning](https://arxiv.org/abs/1610.02413) and [Fairness Beyond Disparate Treatment & Disparate Impact: Learning Classification without Disparate Mistreatment](https://arxiv.org/abs/1610.08452) (*equalizing odds*)\r\n\r\n*   Dec 13: **Fairness Mechanisms**\r\n\r\n      Understanding the different techniques for ensuring fairness in classification.\r\n\r\n      ​    *Readings* (preprocessing and detection)\r\n    * [Detecting discriminatory rules in a rule-based system](http://pages.di.unipi.it/ruggieri/Papers/tkdd.pdf)\r\n    * [Detecting discriminatory black box decision-making](https://arxiv.org/abs/1412.3756) (and repairing it)\r\n    * [Data preprocessing techniques for classification without discrimination](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0ahUKEwiLtYnNit7QAhWHiVQKHcUaAE8QFggkMAE&url=https%3A%2F%2Fpdfs.semanticscholar.org%2F1a43%2Fd5a8f3dd82a138c92911befba05ae98add27.pdf&usg=AFQjCNHwZ1vsGzJRLsbv4QoW-gLX3DIyCg&sig2=0TikurXGq184Xoqi7O6eMw). \r\n\r\n    ​    *Readings* (building a better classifier)\r\n\r\n    * [Discrimination Aware Decision Tree Learning](http://wwwis.win.tue.nl/~tcalders/pubs/ICDM2010KCP.pdf)\r\n    * [Three Naive Bayes Approaches for Discrimination-Free Classification](https://pdfs.semanticscholar.org/a087/d3893af0276fe3b41924087670b03997f7af.pdf)\r\n    * [A Confidence-Based Approach for Balancing Fairness and Accuracy](https://arxiv.org/abs/1601.05764)\r\n    * [Learning Fair Representations](https://www.cs.toronto.edu/~toni/Papers/icml-final.pdf)\r\n\r\n*   Dec 14: **Fairness Mechanisms (continued)**\r\n\r\n*   Dec 15: **Belief Systems**\r\n\r\n      Axiomatic foundations of fairness and justice. Rawls and beyond.\r\n\r\n*   Dec 16: **Accountability**\r\n\r\n      Probing black-box decision-makers: estimating influence of features.\r\n\r\n*   Dec 17: **Interpretability**\r\n\r\n      Building interpretable models.\r\n\r\n*   Dec 18: **Fairness, Accountability and Transparency in other areas of comuter science**\r\n\r\n      Beyond classification: unsupervised learning and representations\r\n\r\n*   Dec 19: **The Broader View**\r\n\r\n      The influence of algorithmic decision-making in the law and policy. \r\n\r\n*   Dec 20\r\n\r\n### Contact\r\nEmail me at [suresh@cs.utah.edu](mailto:suresh@cs.utah.edu)",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}