---
subtitle: "Lecture 3: Fairness Mechanisms"
title: "GIAN Course on Fairness, Accuracy and Transparency in Machine Learning"
author: "Suresh Venkatasubramanian"
date: "Dec 14, 2016"


---

# Fixing the algorithm

- decision trees: change the split criteria to use entropy based on the protected attribute as well. Doesn't seem to work unless you also relabel the leaves after the fact. 
- Naive Bayes based method: very specific to NB, but has one interesting idea: 
  - train two classifiers, one for one group and one for another. And then run separate decisions for each group. This could very well be illegal. 
- Regularize with a "prejudice penality" that measures information flow beterrn sensitive attribute $S$ and class variable $C$. 
- LFR: define three terms in the cost function. 
  - The first one handles statistical parity: difference between assignments to prototype class $k$ from positive data and assignments to class $k$ from negative group ID are similar for all k. This is averaged over the points, and the difference is averaged over the classes. Note that this is based on a "softmax" assignment of points to clusters. 
  - Second term asks that representation of x in prototype is good. 
  - Third term asks that predictions are accurate. The key is that each predicted value is computed by fixing a prediction for each class, and then averaging over membership in the class. 
- Kun et al generalize the idea of shifting the decision boundary with theoretical backing. They argue that by shifting the boundary by a tuned parameter $\lambda$ in order to maintain statistical parity, they retain certain accuracy characteristics of the original data, relying on results from boosting. 
- Gummadi et al: an interesting idea of covariance to capture mutual information between sensitive attribute and decision boundary. And a neat way to encode disparate mistreatment via this. 

# Modifying the output