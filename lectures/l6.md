---
subtitle: "Lecture 6: Interpretability, continued"
title: "GIAN Course on Fairness, Accuracy and Transparency in Machine Learning"
author: "Suresh Venkatasubramanian"
date: "Dec 18, 2016"
---

## 

brief summary of recidivism work:

9 methods: 

* SLIM
* Decision treesX3
* logistic regression
* random forests
* SVM
* Adaboost

6 tasks:

* arrest
* drug
* domestic violence
* general violence
* sexual violence
* fatal crime

bottom line: SLIM performs as well as baselines with good accuracy/interpretability tradeoffs. 

# Reverse engineer something interpretable

in the second class of methods, we build a model using whatever methods we have at our disposal. But then we want to reverse engineer an explanation from the model. One of the advantages of building a model first is that we can look at the model and extract information from it. We can also query the model with new data to get more insight. We'll see all these approaches here. 

## from linear classifiers

Find a classifier $w^\top x = \gamma$. now we need to find a range of values $l_i, u_i$ so that for x in that range, we have $w^\top x < \gamma$. 

Trick. transform the problem into one in the positive hypercube. Let $T_{ii} = \frac{\text{sign}(w_i)}{u_i - l_i}$. And let b have coordinates of the form $l_i$ if $w_i \le 0$ and $u_i$ otherwise. Set $y = T(x-b)$. We can show that $y_i$ is always between 0 and 1, and morever, if we look at the plane $w^\top x = \gamma$, it becomes $w^\top (T^{-1} y + b) = \gamma$, which simplifies to $w'^\top y = 1$ 

## from SVMs and neural nets (and the credit score example)

Suppose we don't have an easy geometric characterization of the model: for example we might have an SVM. 

1. Train an SVM on the data
2. Relabel the training data with the labels produced by the SVM
3. Build a decision tree on the resulting data set. 

If there isn't enough data to build a full tree (i.e with homogeneous leaves), then the system can also query the classifier with specifically chosen new inputs. This is reasonable because now we're not making up data, but we're probing a function. At this point, the complex model is now "nature". 

## a general approach to extracting a simple model from a complex one. 

Explain approach based on resampling in vicinity

idea of explaining with representatives. 

# Fairness in other settings

## word2vec kalai et al

1. Find the "she" -> "he" axis in the embedding and project occupations onto this axis. ask Turkers to rate whether they thought the occupation had a gender stereotype. Check correlation with embedding. 

2. Find stereotypical word pairs: she:he as cupcakes:pizza (by finding words that have the same direction - as measured by cosine similarity) as the she-he direction, for close enough words. Ask Turkers whether they thought the analogy was fair or gender biased. 

   * nurse-surgeon
   * cupcakes-pizza
   * diva-superstar
   * vocalist-guitarist
   * housewife-shopkeeper
   * giggle-chuckle
   * interior designer-architect

   unbiased analogies:

   * queen-king
   * sister-brother
   * convent-monastery
   * mother-father

3. They found a "gender direction" by identifying pairs of words that captured gender polarity and averaging them. They looked at the principal components to verify that this was indeed a clear signal

4. They used this to find direct bias, as well as indirect bias. 

