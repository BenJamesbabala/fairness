---
subtitle: "Lecture 5: Interpretability"
title: "GIAN Course on Fairness, Accuracy and Transparency in Machine Learning"
author: "Suresh Venkatasubramanian"
date: "Dec 17, 2016"
---

# Introduction

Why do we care about interpretability

* medical applications
* trust between user and system
* legal requirements for explainability
* new insights
* problem of predicting on unseen instances: we don't know the classes and need to trust it
* story of tank prediction (enemy/friendly) becoming sunlight prediction

## Some examples

### Decision trees

Decision trees are the canonical example of a plausibly interpretable model. Each local decision is governed by a single attribute, and each test example generates a distinct explanation from the "dictionary" of root-to-leaf paths. While the tree depth might be large, the filtering builds up progressively, so it's possible to determine which attributes are more influential in making the decision: for example, if they're higher up in the tree or have more leafs in their subtree. 

The catch is that a decision tree might often need to repeat subtrees if a decision splits only on one value of a multivalued attribute

> do the example from the comprehensible classification paper, where the rule is: old + smoking -> disease and positive blood test -> disease. Then if the first node is age, and its three-valued (old, middle aged, young) then the latter values will have repeated subtrees for the positive blood test. 

### k-NN classifiers

(recall the definition of a k-NN classifier)

k-NN classifiers also present a version of interpretability, not via rules but via prototypes. this is more a netflix-style recommendation in that the class label is assigned because others like it were assigned the same class as well (and these can usually be presented to the user). The problem with this is that the near neighbors may not necessarily be that close, and so the prototypes might look different enough from the test example that it might not be obvious why there's a common thread between them. 

## Measuring interpretability

Rather than attempt to examine the effectiveness of standard models, we could lay out desirable characteristics for a model that's interpretable. 

### Model size

Dating back to Occam's razor, one of the most fundamental notions of interpretability relates to model complexity. The idea is that if we can build a very concise model, then we reduce the cognitive overload on the analyst and  isolate the key components that control the prediction. Model complexity can be measured in a variety of ways. For example, a linear model is less complex than a quadratic model because it requires fewer parameters. A shallow decision tree is less complex than a deep decision tree because its height (and the number of nodes) is less. 

But saying that we want a concise model is not exactly the same as requiring small model complexity. For example, a standard approach to limiting model complexity is to write down a loss function for the learning task with a regularizer that controls the model complexity. But this kind of learning encourages a very compressed model. Imagine a compressed decision tree that uses very non-standard variables to attempt a quick drill down to the desired leaf node. User studies reveal (*cite*) that people find such compressed models *less* interpretable, possibly because the need to compress them makes them more cryptic and magical. 

### Sparsity

A notion that is related to model complexity is sparsity. The idea of sparsity is that we don't restrict the model itself, but we do restrict the number of nonzero terms used to describe it. For example, we might require that a lienar classifier only has 5 nonzero elements in it, basically saying that we want the final decision to be expressed in terms of only 5 features. 

### Monotonicity

Finally, it has been found in user studies (*cite*) that users like decisions that are monotonic, in that an increase in weight of an attribute only strengthens the decision (or vice versa). Another "cross variable" kind of monotonicity is that it is harder to understand a model where variables may counter each others' influence instead of all features "pulling together" in some way. 

# Strategies for interpretability

So what strategies can we adopt to get models we can understand. The literature describes two basic techniques for interpretable learning: 

1. Define a class of models that are deemed interpretable and only use those to build your model
2. Build a model anyway you want, but also build an *explanation* that is more transparent. 

We'll look at these two approaches one by one. 

## Build an interpretable model

### Sparsity

We've talked about sparsity as a way to create interpretability, by limiting the number of parameters actually used by the model. There are many examples of sparsity in model creation, and we'll focus on one of the most standard ones, the LASSO. 

Let's look at an ordinary least squares. In this problem, we're given a collection of pairs $(\vec{x}, y)$ and are searching for a model of the form 

$$ y= \langle \vec{b}, \vec{x} \rangle + c$$

In matrix form, this means that we wish to minimize 

$$ \|Y - Xb\| $$

(the offset c can be computed separately). 

The solution to this is a vector $b$, and in general $b$ might have all nonzero elements, Suppose we want to impose a sparsity constraint on $b$. we might say something like minimize  $ \|Y - Xb\| $ where $b$ has $t$ nonzero coefficients. This latter constraint can be written as

$$ \|b\|_0 \le t $$

But this is not a convex constraint, and makes it difficult to solve the problem. A way to get a simlar effect without the nonconvex constraint is to instead use the $\ell_1$ norm: so we say 

$$\|b\|_1 \le t$$

(*explain the intuition for why this is convex and relaxes the $l_0$ norm*). 

The LASSO is harder to minimize than ordiinary least squares. While it is still convex, it has sharp corners and needs a different approach. 

### monotonicity

One way of expressing the idea of monotonicity is that we should use positive coefficients to capture parameters of the model. for example, in our linear regression problem, we might to restrict all the beta values to be more than zero. 

This idea plays out in an interesting way in the problem of finding low-rank representations of a matrix. 

(*explain SVD*)

describe NMF. use document term matrix as example

### SLIM models

there's a more general approach to building interpretable models, and this is due to Ustun and Rudin. 

explain the SLIM formulation and each term. Explain what the resulting "scoring rule" looks like. 

also talk about example from recidivism



