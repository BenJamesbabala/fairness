---
subtitle: "Lecture 2: Fairness Mechanisms"
title: "GIAN Course on Fairness, Accuracy and Transparency in Machine Learning"
author: "Suresh Venkatasubramanian"
date: "Dec 13, 2016"
bibliography: fatcs.bib

---

# Introduction

In the last lecture, we reviewed the use of machine learning in a number of "fairness-critical" environments and talked about definitions of nondiscrimination and fairness that have been proposed over the years. In this lecture, we'll look at *mechanisms* for achieving the desired goals: how can we build systems that will be fair and unbiased in the different ways we've defined so far.

Not surprisingly, different definitions will call for different solutions, and we'll have to discuss them separately. There are three broad classes of techniques that apply independently of the measure chosen and we'll organize our exploration around these three ideas.

These three approaches are:

* modifying the training data so that outcome of the classifier will be fair even if we don't modify the classifier.
* modifying the way the classifier is trained (often by modifying the cost function)
* looking at the classifier outcomes and modifying the algorithm based on it.

# Modifying the data

Modifying the training data has certain attractive properties. First of all, it's an acknowledgement that we might not always realize when our data source is biased, so running the data through a "de-biaser" might just be a useful sanity check. Secondly, it creates a clean separation of concerns: we can build a model that's as good as we want, using whatever tools we have at our disposal, without worrying about issues of bias or fairness at all.

## Why is data modification acceptable?

But there's a larger concern with modifying the data. Are we tampering with truth in the process? If the data is something we've collected, is changing it tantamount to forcing a conclusion that we want, rather than letting the data tell us what we need?

There's no easy answer to this question, but there's a way to look at this that might be helpful. There are two "properties" of the data that we appear to be concerned with:

1. Is the data real? (i.e has been collected from actual sources)
2. Is the data unbiased?

Let us consider the four ways to answer these two questions.

* (NO, NO): This is easy: the data is basically useless!
* (YES, YES): Again this is easy, for the opposite reason: the data is high quality and has been collected with care, and so we can run our models without worry.

The interesting cases occur when one of these questions has a NO answer and the other has a YES answer. The situation we are concerned with is the case (YES, NO) when the data collection process is real but might have introduced bias. In such a scenario, we have two choices. We can either try to collect more diverse data to compensate for the bias (moving us to a (YES, YES) scenario) or we can try to eliminate the bias by data modification (moving us to a (NO, YES) scenario).

The hesitation we feel with this latter strategy is that we no longer have a claim that our data is real: however, this must be weighed against the sense that the data is not unbiased. In other words, *unskewing* the data is dangerous if we think we're going from a (YES, YES) to a (NO, NO) scenario. But it's an acceptable compromise if we're actually going from a (YES, NO) to a (NO, YES) scenario.

This discussion is somewhat cartoonish, in that I'm presenting a binary way of thinking about the world. Of course, what we'd like in practice is a way to make the "data is biased" problem go away without changing the "data is real" fact by too much. We will see this trade off appear naturally in the methods we describe here.

## Modification strategies

Each item in the data consists of two parts: the features $X$ and the label $y$. Conceivably, either or both of these could be modified. We'll discuss strategies that try both of these ideas.

### Modifying the classifier labels

One way we could modify classifier labels is to recognize that points that lie on or close to the decision boundary are uncertain in the labeling to begin with, and so we could change their labels without too much harm. This is the key behind the first idea in the paper by Kamiran and Calders [@kamiran_data_2012]. Let $X$ denote the data and let $y_i$ be the label for the point $\vec{x}_i$. Further, let $g(\vec{x})$ denote the value of the protected attribute (we will assume that $g(\vec{x}) = 0$ denotes the minority group).

Their approach works in two steps. Firstly, they build a ranker $r(\vec{x})$ that for each point $\vec{x}$ estimates its likelihood of getting a positive label. For example, in their experiments they use a naive Bayes ranker, a decision-tree ranker as well as a $k$-nn classifier-based ranker. Next, they estimate the number of points from the protected group that need to swap labels in order to ensure an appropriate amount of statistical discrepancy. This estimation accounts for flipping labels for the same number of elements in the unprotected category to make sure that the overall class distribution remains the same (and so the overall change is minimal). In particular, assuming that they want a perfectly balanced data set (no statistical discrepancy), then the number of points whose labels they need to change equals

$$f =  |\{(\vec{x}, y) \mid y = 1, g(\vec{x}) = 1 \}| - |\{ (\vec{x}, y) \mid y = 1, g(\vec{x}) = 0\} |$$

Each point with $g(\vec{x})=0$ and $y = 0$ is added to a list in decreasing order of the rank $r(\vec{x})$. Similarly, each point with $g(\vec{x}) = 1$ and $y = 1$ is added to another list in *increasing* order of $r(\vec{x})$. Then pairs of points are taken off the top of each list and their class labels are swapped till the bias goes to zero. Finally, a new classifier is trained on this new data.

The authors discuss alternate versions of this idea. For example, if changing labels is a problem, an alternative is to change the weights of points. For example, increasing the weight of points with $g(\vec{x})=0$ and $y = 1$  will encourage a classifier to label more points this way. This approach works with classification algorithms that take weights into account: if not, the authors also suggest a sampling scheme that creates a skewed but unweighted data set that has the same effect.

### Modifying the feature values

The other approach to modifying the input data is to change the data itself rather than the labels. This is the approach taken by Feldman et al [@feldman_certifying_2015] for the purpose of avoiding disparate impact. The challenge faced in this work is that the classifier itself is a black box, and so unlike in the previous section where one might attempt to look at the structure of the classification, this can't be done here.

So the only strategy is to essentially cut off the information flow between the protected attribute and the output classification, both directly or even indirectly via other attributes. To this end, the authors show two things. Firstly, they show that the ability of the (black-box) classifier to incur disparate impact by delivering a biased decision can be directly related to the ability to predict the protected attribute from the remaining features of the data set. Secondly, they use this result to argue that it is sufficient therefore to modify the data set so that the protected attribute *cannot* be predicted from the remaining attributes. This ensures that no classifier will be able to generate a biased outcome.

In these notes, we will discuss only the mechanics of performing the repair, and only briefly. As before, let $X$ be the data set, and suppose that each item consists of the group attribute $g$ and the remaining (unprotected) attributes $u$. Then the goal is to modify $X$ so that we cannot predict the value of $g$ from $u$. Consider the conditional distributions $\Pr(u \mid g)$ for each value of $g$. One way to ensure that we cannot predict $g$ is to ensure that all of these conditional distributions are the same. While it would be easy to set all the conditional distributions to some fixed distribution, this would involve large-scale data modification that might be unsuitable for the underlying task of classification.

Fix a metric space $(X, d)$ and suppose that $p,q \in \Delta(X)$ are probability distributions over $X$. We can define the *earthmover distance* between $p$ and $q$ as the quantity

$$ d_{EM}(p,q)  = \min_{\pi} \sum_{x, y \in X} \pi(x,y) d(x,y) $$

where $\pi(x,y)$ is a joint distribution on $X \times X$ whose marginals are $p$ and $q$ (ie. $\sum_{y \in X} \pi(x,y) = p(x)$ and $\sum_{x \in X}\pi(x,y) = q(y)$). Then our goal is to ask for a fixed probability distribution $r$ such that

$$ \sum_{v \in \text{Domain}(g)} d_{EM}(\Pr(u | g = v), r) $$

is minimized. The authors then show that for one-dimensional distributions this can be expressed simply in terms of inverse cumulative density functions of the given distributions.

# References
