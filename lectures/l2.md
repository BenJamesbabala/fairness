---
subtitle: "Lecture 2: Fairness Mechanisms"
title: "GIAN Course on Fairness, Accuracy and Transparency in Machine Learning"
author: "Suresh Venkatasubramanian"
date: "\today"

---

# Introduction

In the last lecture, we reviewed the use of machine learning in a number of "fairness-critical" environments and talked about definitions of nondiscrimination and fairness that have been proposed over the years. In this lecture, we'll look at *mechanisms* for achieving the desired goals: how can we build systems that will be fair and unbiased in the different ways we've defined so far. 

Not surprisingly, different definitions will call for different solutions, and we'll have to discuss them separately. There are three broad classes of techniques that apply independently of the measure chosen and we'll organize our exploration around these three ideas. 

These three approaches are:

* modifying the training data so that an *oblivious* training procedure will find a fair and accurate classifier. 
* modifying the way the classifier is trained (often by modifying the cost function)
* looking at the classifier outcomes and modifying the algorithm based on this. 

# Modifying the data

There are two ways to modify the input training data so that a classifier doesn't "see" bias. 

1. Modify the training labels so the data set looks balanced. 
   * this requires building a ranker to estimate for each item the likelihood that it will be labeled positive or negative
   * And then moving labels around
   * Alternatively one can reweight points based on the idea that in a biased setting, there should be independence between label and group ID. 
   * Main experimental conclusions: 
2. Modify the data itself to be indistinguishable based on group ID. 
   * our paper.

# Fixing the algorithm

* decision trees: change the split criteria to use entropy based on the protected attribute as well. Doesn't seem to work unless you also relabel the leaves after the fact. 

# Modifying the output